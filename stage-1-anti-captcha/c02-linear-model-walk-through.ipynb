{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have fun with linear model\n",
    "\n",
    "Author: hdup  \n",
    "My contact info:  \n",
    "hdup [huangdan@youhujia.com](mailto:huangdan@youhujia.com)  \n",
    "evitself [evitself@gmail.com](mailto:evitself@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play with linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnt = 100\n",
    "\n",
    "train_X = np.linspace(-3.0, 3.0, num=sample_cnt, dtype=np.float32).reshape((sample_cnt, 1))\n",
    "train_y = train_X * 0.375 + 1.1\n",
    "\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1), define model, cost, and derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_model(X, W, b):\n",
    "    \"\"\"\n",
    "    W is (k, 1) matrix\n",
    "    b is scalar\n",
    "    X is (m, k) matrix\n",
    "    \"\"\"\n",
    "    return np.matmul(X, W) + b\n",
    "\n",
    "def mse_cost(h, y):\n",
    "    \"\"\"\n",
    "    y is (m, 1) matrix\n",
    "    h is (m, 1) matrix\n",
    "    \"\"\"\n",
    "    diff = h - y\n",
    "    return 0.5 * np.matmul(diff.transpose(), diff).mean()\n",
    "\n",
    "def mse_cost_dev(X, y, h):\n",
    "    \"\"\"\n",
    "    X is (m, k) matrix\n",
    "    y is (m, 1) matrix\n",
    "    h is (m, 1) matrix\n",
    "    \"\"\"\n",
    "    diff = h - y\n",
    "    return (np.matmul(diff.transpose(), X) / X.shape[0]).transpose(), (diff.mean())\n",
    "\n",
    "def gd_update(X, y, h, W, b, lr=0.01):\n",
    "    d_W, d_b = mse_cost_dev(X, y, h)\n",
    "    return (W - lr * d_W), (b - lr * d_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2), training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(1).reshape((1, 1))\n",
    "b = np.random.randn()\n",
    "\n",
    "# batch learning\n",
    "for epoch in range(0, 1000):\n",
    "    h = linear_model(train_X, W, b)\n",
    "    W, b = gd_update(train_X, train_y, h, W, b, lr=0.01)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        cur_cost = mse_cost(h, train_y)\n",
    "        print('epoch: {0}, cost:{1}, W:{2}, b:{3}'.format(epoch + 1, cur_cost, W, b))\n",
    "\n",
    "# finish\n",
    "predictions = linear_model(train_X, W, b)\n",
    "final_cost = mse_cost(predictions, train_y)\n",
    "print('training finished!')\n",
    "print('final cost: {0}, W: {1}, b: {2}'.format(final_cost, W, b))\n",
    "\n",
    "# then plot some curves\n",
    "plt.plot(train_X, train_y, 'r+', label='training')\n",
    "plt.plot(train_X, predictions, 'b--', label='fitted')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression (generalized linear model) with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1), define model, cost, and derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.0000001\n",
    "\n",
    "def sigmoid(g):\n",
    "    return 1.0 / (1.0 + np.exp(-g))\n",
    "\n",
    "def logistic_model(X, W, b):\n",
    "    return sigmoid(linear_model(X, W, b))\n",
    "\n",
    "def log_cost(h, y):\n",
    "    return -(y * np.log(h + epsilon) + (1.0 - y) * np.log(1.0 - h + epsilon)).sum(axis=0) / y.shape[0]\n",
    "\n",
    "def log_cost_dev(X, y, h):\n",
    "    diff = h - y\n",
    "    return ((diff * X).sum(axis=0) / y.shape[0]), (diff.sum(axis=0) / y.shape[0])\n",
    "\n",
    "def gd_update(X, y, h, W, b, lr=0.01):\n",
    "    d_W, d_b = log_cost_dev(X, y, h)\n",
    "    return (W - lr * d_W), (b - lr * d_b)\n",
    "\n",
    "def binary_accuracy(h, y, threshold=0.5):\n",
    "    right_cnt = 0\n",
    "    for cid in range(0, sample_cnt):\n",
    "        if (y[cid][0] > 0.5) == (h[cid][0] > threshold):\n",
    "            right_cnt += 1\n",
    "    return right_cnt / sample_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2), prepare label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_binary = np.array([1.0 if i > 0.5 else 0 for i in train_y]).reshape(sample_cnt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3), training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(1).reshape((1, 1))\n",
    "b = np.random.randn()\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "# batch learning\n",
    "for epoch in range(0, 10000):\n",
    "    h = logistic_model(train_X, W, b)\n",
    "    W, b = gd_update(train_X, train_y_binary, h, W, b, lr=0.5)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        cur_cost = log_cost(h, train_y_binary)\n",
    "        acc = binary_accuracy(h, train_y_binary, threshold=threshold)\n",
    "        print('epoch: {0}, cost: {1}, W: {2}, b: {3}, acc: {4}'.format(epoch + 1, cur_cost, W, b, acc))\n",
    "\n",
    "# finish\n",
    "predictions = logistic_model(train_X, W, b)\n",
    "final_cost = log_cost(predictions, train_y_binary)\n",
    "final_acc = binary_accuracy(predictions, train_y_binary, threshold=threshold)\n",
    "print('training finished!')\n",
    "print('final cost: {0}, W: {1}, b: {2}, acc: {3}'.format(final_cost, W, b, final_acc))\n",
    "\n",
    "# then plot some curves\n",
    "plt.plot(train_X, train_y_binary, 'r-', label='training')\n",
    "plt.plot(train_X, predictions, 'b-', label='fitted')\n",
    "plt.axhline(y=threshold, color='g', linestyle='-', label='threshold')\n",
    "plt.grid(True)\n",
    "plt.ylim([-0.2, 1.2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Time to have fun with cracking captcha with LR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1), load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import corpus, corpus_len, show_img, prepare_data, one_hot_to_label, showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('./data/data.npz')\n",
    "train_X=train_data['X']\n",
    "train_y=train_data['y']\n",
    "# take first 10000 samples\n",
    "#train_X = train_X[0:10000,:]\n",
    "#train_y = train_y[0:10000,:]\n",
    "print('shape X {0}, y {1}'.format(train_X.shape, train_y.shape))\n",
    "\n",
    "validate_data = np.load('./data/val.npz')\n",
    "validate_X=validate_data['X']\n",
    "validate_y=validate_data['y']\n",
    "print('shape X {0}, y {1}'.format(validate_X.shape, validate_y.shape))\n",
    "\n",
    "test_data = np.load('./data/test.npz')\n",
    "test_X=test_data['X']\n",
    "test_y=test_data['y']\n",
    "print('shape X {0}, y {1}'.format(test_X.shape, test_y.shape))\n",
    "\n",
    "train_X, train_y = prepare_data(train_X, train_y)\n",
    "validate_X, validate_y = prepare_data(validate_X, validate_y)\n",
    "test_X, test_y = prepare_data(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2), let's take character A as example\n",
    "\n",
    "You should know one-vs-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnt = train_X.shape[0]\n",
    "feature_cnt = train_X.shape[1] * train_X.shape[2]\n",
    "\n",
    "train_X_a = train_X.reshape((sample_cnt, feature_cnt))\n",
    "validate_X_a = validate_X.reshape((validate_X.shape[0], feature_cnt))\n",
    "test_X_a = test_X.reshape((test_X.shape[0], feature_cnt))\n",
    "\n",
    "train_y_a = train_y[:,0:1]\n",
    "validate_y_a = validate_y[:,0:1]\n",
    "test_y_a = test_y[:,0:1]\n",
    "\n",
    "print(train_X_a.shape)\n",
    "print(train_y_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# redefine linear model to support matrix\n",
    "epsilon = 0.0000001\n",
    "\n",
    "def linear_model(X, W, b):\n",
    "    return np.matmul(X, W) + b\n",
    "\n",
    "def sigmoid_stable(x):\n",
    "    if x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1.0 / (1.0 + z)\n",
    "    else:\n",
    "        z = np.exp(x)\n",
    "        return z / (1.0 + z)\n",
    "\n",
    "def sigmoid(g):\n",
    "    return np.vectorize(sigmoid_stable, otypes=[np.float32])(g)\n",
    "\n",
    "def logistic_model(X, W, b):\n",
    "    return sigmoid(linear_model(X, W, b))\n",
    "\n",
    "def log_cost(h, y):\n",
    "    return -(y * np.log(h + epsilon) + (1.0 - y) * np.log(1.0 - h + epsilon)).sum(axis=0) / y.shape[0]\n",
    "\n",
    "def log_cost_dev(X, y, h):\n",
    "    diff = h - y\n",
    "    return (np.matmul(diff.transpose(), X) / y.shape[0]), diff.mean()\n",
    "\n",
    "def gd_update(X, y, h, W, b, lr=0.01):\n",
    "    d_W, d_b = log_cost_dev(X, y, h)\n",
    "    return (W - lr * d_W.reshape((d_W.shape[0], 1))), (b - lr * d_b)\n",
    "\n",
    "def binary_precision(h, y, threshold=0.5):\n",
    "    relevant = 0\n",
    "    selected = 0\n",
    "    for cid in range(0, y.shape[0]):\n",
    "        if h[cid][0] > threshold:\n",
    "            selected += 1\n",
    "            if y[cid][0] > 0.5:\n",
    "                relevant += 1                \n",
    "    return 0 if selected==0 else relevant / selected\n",
    "\n",
    "def binary_confusion_matrix(h, y, threshold=0.5):\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "    for cid in range(0, y.shape[0]):\n",
    "        if h[cid][0] > threshold:            \n",
    "            if y[cid][0] > 0.5:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "        else:\n",
    "            if y[cid][0] > 0.5:\n",
    "                false_neg += 1\n",
    "            else:\n",
    "                true_neg += 1\n",
    "    pc = true_pos / (true_pos + false_pos)\n",
    "    rc = true_pos / (true_pos + false_neg)\n",
    "    f1 = 2.0 * (pc * rc) / (pc + rc)\n",
    "    return pc, rc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3), training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "W = np.array([np.random.randn() for i in range(0, feature_cnt)]).astype(np.float32).reshape((feature_cnt, 1))\n",
    "b = np.random.randn()\n",
    "\n",
    "epoch_arr = []\n",
    "cost_arr = []\n",
    "metric_arr = []\n",
    "\n",
    "step = 20\n",
    "batch_size = 20\n",
    "\n",
    "# mini-batch learning\n",
    "batch_blocks = sample_cnt / batch_size\n",
    "for epoch in range(0, 1000):\n",
    "    # launch mini-batch\n",
    "    batch_start = 0    \n",
    "    while(batch_start + batch_size < sample_cnt):\n",
    "        batch_X = train_X_a[batch_start:batch_start+batch_size,:]\n",
    "        batch_y = train_y_a[batch_start:batch_start+batch_size,:] \n",
    "        h = logistic_model(batch_X, W, b)\n",
    "        W, b = gd_update(batch_X, batch_y, h, W, b, lr=0.01)\n",
    "        batch_start += batch_size\n",
    "        \n",
    "    # eval epoch\n",
    "    if (epoch + 1) % step == 0:\n",
    "        h = logistic_model(validate_X_a, W, b)\n",
    "        cur_cost = log_cost(h, validate_y_a)\n",
    "        cur_conf = binary_confusion_matrix(h, validate_y_a, threshold=threshold)        \n",
    "        print('epoch: {0}, cost: {1}, val_conf: {2}'.format(epoch + 1, cur_cost, cur_conf))\n",
    "        epoch_arr.append(epoch + 1)\n",
    "        cost_arr.append(cur_cost)\n",
    "        metric_arr.append(cur_conf[2])\n",
    "\n",
    "# finish\n",
    "predictions = logistic_model(train_X_a, W, b)\n",
    "final_cost = log_cost(predictions, train_y_a)\n",
    "final_conf = binary_confusion_matrix(predictions, train_y_a, threshold=threshold)\n",
    "print('training finished!')\n",
    "print('final cost: {0}, conf: {1}'.format(final_cost, final_conf))\n",
    "\n",
    "# calculate test conf\n",
    "test_h = logistic_model(test_X_a, W, b)\n",
    "test_cost = log_cost(test_h, test_y_a)\n",
    "test_conf = binary_confusion_matrix(test_h, test_y_a, threshold=threshold)\n",
    "print('test cost: {0}, conf: {1}'.format(test_cost, test_conf))\n",
    "\n",
    "# plot learning curve\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(epoch_arr, cost_arr, 'r-', label='cost')\n",
    "ymax = np.max(cost_arr)\n",
    "plt.ylim([0.0, ymax * 1.05])\n",
    "ax1.set_ylabel('cost', color='r')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epoch_arr, metric_arr, 'b-', label='f1')\n",
    "ymin = np.min(metric_arr)\n",
    "plt.ylim([ymin-0.02 if ymin-0.02 > 0.0 else 0.0, 1.0])\n",
    "ax2.set_ylabel('f1', color='b')\n",
    "\n",
    "plt.axvline(x=0, color='g', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.xlim([-50, epoch + 50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
